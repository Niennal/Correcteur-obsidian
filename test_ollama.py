#!/usr/bin/env python3
"""
Script de test pour vÃ©rifier la connexion Ã  Ollama
"""
import sys
from langchain_ollama import ChatOllama


def test_ollama_connection(model: str = "llama3.1:8b"):
    """Teste la connexion Ã  Ollama."""
    print(f"ðŸ§ª Test de connexion Ã  Ollama avec le modÃ¨le: {model}")
    print("=" * 70)

    try:
        # Initialiser le modÃ¨le
        print(f"\n1ï¸âƒ£ Initialisation du modÃ¨le {model}...")
        llm = ChatOllama(
            model=model,
            base_url="http://localhost:11434",
            temperature=0.7,
        )
        print("âœ… ModÃ¨le initialisÃ©")

        # Test simple
        print(f"\n2ï¸âƒ£ Envoi d'un message de test...")
        from langchain_core.messages import HumanMessage

        messages = [
            HumanMessage(content="RÃ©ponds juste 'OK' si tu me reÃ§ois bien.")
        ]

        response = llm.invoke(messages)
        print(f"âœ… RÃ©ponse reÃ§ue: {response.content}")

        # Test avec tool call
        print(f"\n3ï¸âƒ£ Test des tool calls...")

        from langchain_core.tools import tool

        @tool
        def get_weather(location: str) -> str:
            """Obtenir la mÃ©tÃ©o pour un lieu donnÃ©."""
            return f"Il fait beau Ã  {location}"

        llm_with_tools = llm.bind_tools([get_weather])

        messages = [
            HumanMessage(content="Quelle est la mÃ©tÃ©o Ã  Paris?")
        ]

        response = llm_with_tools.invoke(messages)

        if hasattr(response, 'tool_calls') and response.tool_calls:
            print(f"âœ… Tool call dÃ©tectÃ©: {response.tool_calls[0]['name']}")
        else:
            print(f"âš ï¸ Pas de tool call dÃ©tectÃ© (normal pour certains modÃ¨les)")
            print(f"   RÃ©ponse: {response.content[:100]}...")

        print("\n" + "=" * 70)
        print("âœ… Tous les tests ont rÃ©ussi!")
        print("=" * 70)
        print("\nðŸ’¡ Conseils:")
        print("  - Si les tool calls ne fonctionnent pas bien, essayez: mistral-nemo:12b")
        print("  - VÃ©rifiez que vous avez assez de RAM disponible")
        print("  - Utilisez 'ollama ps' pour voir les modÃ¨les chargÃ©s en mÃ©moire")

        return True

    except Exception as e:
        print("\n" + "=" * 70)
        print(f"âŒ Erreur: {e}")
        print("=" * 70)
        print("\nðŸ”§ Solutions possibles:")
        print("  1. VÃ©rifiez qu'Ollama est lancÃ©: 'ollama serve'")
        print("  2. VÃ©rifiez que le modÃ¨le est installÃ©: 'ollama list'")
        print(f"  3. Si besoin, installez le modÃ¨le: 'ollama pull {model}'")
        print("  4. VÃ©rifiez la connexion: 'curl http://localhost:11434/api/version'")
        return False


def list_available_models():
    """Liste les modÃ¨les disponibles."""
    import subprocess

    print("\nðŸ“‹ ModÃ¨les Ollama installÃ©s:")
    print("=" * 70)
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            check=True
        )
        print(result.stdout)
    except Exception as e:
        print(f"âŒ Impossible de lister les modÃ¨les: {e}")


if __name__ == "__main__":
    print("ðŸ¤– Test de connexion Ollama pour Obsidian Multi-Agent")
    print()

    # Lister les modÃ¨les disponibles
    list_available_models()

    # Test avec le modÃ¨le par dÃ©faut
    model = sys.argv[1] if len(sys.argv) > 1 else "llama3.1:8b"

    success = test_ollama_connection(model)

    sys.exit(0 if success else 1)
